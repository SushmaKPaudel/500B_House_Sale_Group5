import numpy as np
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import matplotlib as mp1 
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler

1.1. Import dataset and describe characteristics such as dimensions, data types, file types, and import methods used
------------------------------------import csv file in Panda Dataframe------------------------------------------------------
Importing the data as dataframe.We are going to use pd.read_csv to import the file in dataframe.
df =pd.read_csv(r'C:\Users\Hari\Documents\Sushma_data_science\500B\Final_Project\house_sales.csv', sep=",")

## Looking few rows os data at glance 
df.head()

-----------------------------------find dimension of dataframe df----------------------------------------------------------
Shape attribute gives us dimension of dataframe so lets's use .shape to identify number of rows and columns of dataframe.
print(df.shape)

----------------------------------find data types of all columns in dataframe df-------------------------------------------
dttypes attribute tells us data types of each column from the the dataframe.
print(df.dtypes)

-----------------------------------find the extension of file--------------------------------------------------------------
Let's use Python's built in libraray to identyfy the extenstion of file using os.path.splitext().
import os
file_extension = os.path.splitext("house_sales.csv")
print(file_extension)

---------------------------------dataype of each columns in dataframe------------------------------------------------------
numeric_columns = df.select_dtypes(include=['number']).columns
nonnumeric_columns =df.select_dtypes(exclude=['number']).columns

print (numeric_columns)
print (nonnumeric_columns)

------------------------------------------------------------------------------------------------------------------
1.2.Clean, wrangle, and handle missing data.
lets's count null values for each columns

null_counts = df.isnull().sum()

print(null_counts)

4 fields bedrooms, bathrooms, sqft_living and sqft_lot contain null values. Now , lets replace the null values with the mean 
value of each fields as we already know the 4 fields that are missing values are numeric types.

df.fillna(df.mean(numeric_only=True).round(1), inplace=True)

Now, lets verify there are no more null values in those 4 fields.

null_counts2 = df.isnull().sum()
print(null_counts2)
----------------------------------------------------------------------------------------------------------------------
1.3. Transform data appropriately using techniques such as aggregation, normalization, and feature construction

#Normalize columns 
scaler = MinMaxScaler() ##Not sure if its going to useful or not we can remove if we find this not adding any values
df['normalized_sqft_living'] = scaler.fit_transform(df[['sqft_living']])

---------------------------------------------------------------------------------------------------------------------
1.4. Reduce redundant data and perform need-based discretization.
-----------Remove the duplicate records for each primarykey/id from dataframe df--------------------------------

# Check duplicates based on the 'id' column.
duplicates = df.duplicated(subset=['id'], keep=False)

# identyfying dataframe duplicate_rows from the df to get the duplicate rows
duplicate_rows = df[duplicates]

print(duplicate_rows)

-----------------------------------------------
df = df.drop_duplicates(subset=['id'], keep='last') ## we decided to keep last from each dupe id

-------------------------------------------------
Now , leteverify theer are no more duplicates in dataframe.

# Check duplicates based on the 'id' column.
duplicates = df.duplicated(subset=['id'], keep=False)

# identyfying dataframe duplicate_rows from the df to get the duplicate rows
duplicate_rows = df[duplicates]

print(duplicate_rows)


-----------------------------------------------
data =pd.read_csv(r'C:\Users\Hari\Documents\Sushma_data_science\500B\Module6\airline_costs.csv', sep=",")


-----------------------------------------------------------------------------------
*BASED OFF THE DATA ALREADY ALREADY PROVIDED

2.0 Data Analysis and Visualization
2.1 Identify categorical, ordinal, and numerical variables within the data
--------------------Identifying variables -----------------------
categorical_vars = ['waterfront', 'view', 'zipcode']
ordinal_vars = ['grade']
numerical_vars = list(set(df.columns) - set(categorical_vars) - set(ordinal_vars) - {'id', 'date'})

print("Categorical variables:", categorical_vars)
print("Ordinal variables:", ordinal_vars)
print("Numerical variables:", numerical_vars)


2.2 Provide measures of centrality and distribution with visualizations

----------------------numerical summary (statistical distribution) -------------------

numerical_summary = df[numerical_vars].describe()
numerical_summary

-----------------visualizing the distribution of all variables--------------

#first histogram
df[numerical_vars].hist(bins=20, figsize=(20, 15))
plt.show()

#second boxplots
plt.figure(figsize=(20, 15))
for i, var in enumerate(numerical_vars):
    plt.subplot(4, 4, i+1)
    sns.boxplot(x=df[var])
plt.tight_layout()
plt.show()

2.3 Diagnose for correlations between variables and determine independent and dependent variables

correlation_matrix = df[numerical_vars].corr()
correlation_matrix

------------visualize the correlation with Heatmap, to check for multicollinearity-----------

plt.figure(figsize=(15, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

#Extract independent variables with a correlation coefficient greater than 0.5 or less than -0.5 with the target variable 'Price' to build the predictive model.
correlation_with_price = correlation_matrix['price'].sort_values(ascending=False)
print(correlation_with_price)

-------Select variables with strong correlation to price--------------
strong_correlations = correlation_with_price[abs(correlation_with_price) > 0.5]
print(strong_correlations)


2.4 Perform exploratory analysis in combination with visualization techniques to discover patterns and features of interest
---------------------checking for multicollinearity using scatter plot----------
 

  for col1 in numerical_vars:
    for col2 in numerical_vars:
        if col1 != col2:
            plt.figure(figsize=(8, 6))
            sns.scatterplot(x=df[col1], y=df[col2])
            plt.title(f'{col1} vs {col2}')
            plt.show()

-------------------calculating for R-squared-------------------------
  y = df['price']

variables = ['sqft_living', 'sqft_above', 'sqft_living15', 'bathrooms', 'sqft_basement', 
             'bedrooms', 'lat', 'floors', 'yr_renovated', 'sqft_lot', 
             'sqft_lot15', 'yr_built', 'condition', 'long']
  for var in variables:
    X = df[[var]]
    X = sm.add_constant(X)  # Adds a constant (intercept) term to the model
    model = sm.OLS(y, X).fit()
    r_squared = model.rsquared
    r_squared_values[var] = r_squared

  r_squared_df = pd.DataFrame(list(r_squared_values.items()), columns=['Variable', 'R-squared'])

  r_squared_df['R-squared'] = r_squared_df['R-squared'].round(2)
  print(r_squared_df)

  # now that we can see our interest variables based on correlation and R-squared value, 
  # lets do multicollinearity check on our variables of interest

  -----------------Multicollinearity check------------
  
  X = df [['sqft_living', 'sqft_above', 'sqft_living15', 'bathrooms']].dropna()
X = sm.add_constant(X)

vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

print(vif_data)


#having VIF values lower than 5, we are safe from multicollinearity
  and our independent variables are "         feature        VIF
0          const  10.430581
1    sqft_living   4.630557
2     sqft_above   4.070416
3  sqft_living15   2.398817
4      bathrooms   2.102460"


-----------------------------------------------------------------------------------------------------------

*BASED OFF DF ALREADY BEING CLEANED AND IND/DEP VARIABLES BEING DETERIMED



3. Creating a regression model
----------------------------fit the model--------------------------------------------------------
Find the best-fitting line through the data points

model = sm.ols('price ~ sqft_living', data = df).fit()

model1 = sm.ols('price ~ bathrooms', data = df).fit()

model2 = sm.ols('price ~ sqft_above', data = df).fit()

----------------------------print the summary statistics--------------------------
Show a comprehensive summary of the regression model  to understand the relationship between the independent and dependent 
variable 

print(model.summary())

----------------------------calculate predictions--------------------------
Generate predictions based on the fitted model and calculates the predicted value of price based 'sqft_living' using the
regression model

predictions = model.predict(df[['sqft_living']])

predictions1 = model1.predict(df[['bathrooms']])

predictions2 = model2.predict(df[['sqft_above']])

----------------------------calculate evaluation metrics----------------------------------
Asses accuracy of model's predictions using evaluation metrics

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

#price ~ sqft_living
mae = mean_absolute_error(df['price'], predictions)
mse = mean_squared_error(df['price'], predictions)
r2 = r2_score(df['price'], predictions)
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"R² Score: {r2}")

#price ~ bathroom
mae1 = mean_absolute_error(df['price'], predictions1)
mse1 = mean_squared_error(df['price'], predictions1)
r21 = r2_score(df['price'], predictions1)
print(f"Mean Absolute Error (MAE): {mae1}")
print(f"Mean Squared Error (MSE): {mse1}")
print(f"R² Score: {r21}")

#price ~ sqft_above
mae2 = mean_absolute_error(df['price'], predictions2)
mse2 = mean_squared_error(df['price'], predictions2)
r22 = r2_score(df['price'], predictions2)
print(f"Mean Absolute Error (MAE): {mae2}")
print(f"Mean Squared Error (MSE): {mse2}")
print(f"R² Score: {r22}")
------------------------------create scatter plot------------------------------------------------------------------------
Create Scatter Plot of Actual Vs Predicted Prices

plt.figure(figsize=(10, 6))
plt.scatter(df['price'],predictions)
plt.plot([df['price'].min(), df['price'].max()], [df['price'].min(), df['price'].max()], color='red', linestyle='--')
plt.title('Actual vs. Predicted House Prices Based on Living Area (sqft)')
plt.xlabel('Actual Price')
plt.ylabel('predicted price')
plt.show

-----------------------------(optional) make x and y axis show full numbers instead of scientific notation------------------

import matplotlib.ticker as ticker

formatter = ticker.ScalarFormatter(useOffset=False)
formatter.set_scientific(False)

plt.gca().xaxis.set_major_formatter(formatter)
plt.gca().yaxis.set_major_formatter(formatter)

plt.show()



