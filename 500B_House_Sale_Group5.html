import numpy as np
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import matplotlib as mp1 
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler

1.1. Import dataset and describe characteristics such as dimensions, data types, file types, and import methods used
------------------------------------import csv file in Panda Dataframe------------------------------------------------------
Importing the data as dataframe.We are going to use pd.read_csv to import the file in dataframe.
df =pd.read_csv(r'C:\Users\Hari\Documents\Sushma_data_science\500B\Final_Project\house_sales.csv', sep=",")

## Looking few rows os data at glance 
df.head()

-----------------------------------find dimension of dataframe df----------------------------------------------------------
Shape attribute gives us dimension of dataframe so lets's use .shape to identify number of rows and columns of dataframe.
print(df.shape)

----------------------------------find data types of all columns in dataframe df-------------------------------------------
dttypes attribute tells us data types of each column from the the dataframe.
print(df.dtypes)

-----------------------------------find the extension of file--------------------------------------------------------------
Let's use Python's built in libraray to identyfy the extenstion of file using os.path.splitext().
import os
file_extension = os.path.splitext("house_sales.csv")
print(file_extension)

---------------------------------dataype of each columns in dataframe------------------------------------------------------
numeric_columns = df.select_dtypes(include=['number']).columns
nonnumeric_columns =df.select_dtypes(exclude=['number']).columns

print (numeric_columns)
print (nonnumeric_columns)

------------------------------------------------------------------------------------------------------------------
1.2.Clean, wrangle, and handle missing data.
lets's count null values for each columns

null_counts = df.isnull().sum()

print(null_counts)

4 fields bedrooms, bathrooms, sqft_living and sqft_lot contain null values. Now , lets replace the null values with the mean 
value of each fields as we already know the 4 fields that are missing values are numeric types.

df.fillna(df.mean(numeric_only=True).round(1), inplace=True)

Now, lets verify there are no more null values in those 4 fields.

null_counts2 = df.isnull().sum()
print(null_counts2)
----------------------------------------------------------------------------------------------------------------------
1.3. Transform data appropriately using techniques such as aggregation, normalization, and feature construction

#Normalize columns 
scaler = MinMaxScaler() ##Not sure if its going to useful or not we can remove if we find this not adding any values
df['normalized_sqft_living'] = scaler.fit_transform(df[['sqft_living']])

---------------------------------------------------------------------------------------------------------------------
1.4. Reduce redundant data and perform need-based discretization.
-----------Remove the duplicate records for each primarykey/id from dataframe df--------------------------------

# Check duplicates based on the 'id' column.
duplicates = df.duplicated(subset=['id'], keep=False)

# identyfying dataframe duplicate_rows from the df to get the duplicate rows
duplicate_rows = df[duplicates]

print(duplicate_rows)

-----------------------------------------------
df = df.drop_duplicates(subset=['id'], keep='last') ## we decided to keep last from each dupe id

-------------------------------------------------
Now , leteverify theer are no more duplicates in dataframe.

# Check duplicates based on the 'id' column.
duplicates = df.duplicated(subset=['id'], keep=False)

# identyfying dataframe duplicate_rows from the df to get the duplicate rows
duplicate_rows = df[duplicates]

print(duplicate_rows)


-----------------------------------------------
data =pd.read_csv(r'C:\Users\Hari\Documents\Sushma_data_science\500B\Module6\airline_costs.csv', sep=",")


-----------------------------------------------------------------------------------
*BASED OFF THE DATA ALREADY ALREADY PROVIDED

2.0 Data Analysis and Visualization
2.1 Identify categorical, ordinal, and numerical variables within the data
--------------------Identifying variables -----------------------
categorical_vars = ['waterfront', 'view', 'zipcode']
ordinal_vars = ['grade']
numerical_vars = list(set(df.columns) - set(categorical_vars) - set(ordinal_vars) - {'id', 'date'})

print("Categorical variables:", categorical_vars)
print("Ordinal variables:", ordinal_vars)
print("Numerical variables:", numerical_vars)


2.2 Provide measures of centrality and distribution with visualizations

----------------------numerical summary (statistical distribution) -------------------

numerical_summary = df[numerical_vars].describe()
numerical_summary

-----------------visualizing the distribution of all variables--------------

#first histogram to observe the variables disribution
df[numerical_vars].hist(bins=20, figsize=(20, 15))
plt.show()

#second boxplots
plt.figure(figsize=(20, 15))
for i, var in enumerate(numerical_vars):
    plt.subplot(4, 4, i+1)
    sns.boxplot(x=df[var])
plt.tight_layout()
plt.show()

----------Checking for outliers using IQR -------------

# Calculate the IQR
Q1 = df[numerical_vars].quantile(0.25)
Q3 = df[numerical_vars].quantile(0.75)
IQR = Q3 - Q1

# Identify outliers
outliers_iqr = (df[numerical_vars] < (Q1 - 1.5 * IQR)) | (df[numerical_vars] > (Q3 + 1.5 * IQR))

# Verify how many outliers are there
outlier_counts = outliers_iqr.sum()
print("Number of outliers per column:")
print(outlier_counts)
_________________calculating medians for columns of interest -------
     # Calculate medians for columns of interest
modes = df[columns_of_interest].mode()

# Replace outliers with median for specific columns
df_replaced = df.copy()
for col in columns_of_interest:
    if col in outliers_iqr.columns:
        df_replaced.loc[outliers_iqr[col], col] = modes[col]

--------------re-calculating IQR for the updated dataframe----

Q1_updated = df_replaced[numerical_vars].quantile(0.25)
Q3_updated = df_replaced[numerical_vars].quantile(0.75)
IQR_updated = Q3_updated - Q1_updated

# Identify remaining outliers in the updated DataFrame
outliers_iqr_updated = (df_replaced[numerical_vars] < (Q1_updated - 1.5 * IQR_updated)) | (df_replaced[numerical_vars] > (Q3_updated + 1.5 * IQR_updated))

# Verify how many outliers are still there
remaining_outlier_counts = outliers_iqr_updated.sum()
print("Number of remaining outliers per column:")
print(remaining_outlier_counts

------------capping outliers-----------

# Cap the outliers
df_capped = df_replaced.copy()
for col in columns_of_interest:
    if col in outliers_iqr.columns:
        lower_bound = Q1[col] - 1.5 * IQR[col]
        upper_bound = Q3[col] + 1.5 * IQR[col]
        df_capped[col] = df_capped[col].clip(lower=lower_bound, upper=upper_bound)

# Check for remaining outliers after capping
Q1_capped = df_capped[numerical_vars].quantile(0.25)
Q3_capped = df_capped[numerical_vars].quantile(0.75)
IQR_capped = Q3_capped - Q1_capped

outliers_iqr_capped = (df_capped[numerical_vars] < (Q1_capped - 1.5 * IQR_capped)) | (df_capped[numerical_vars] > (Q3_capped + 1.5 * IQR_capped))
remaining_outlier_counts_capped = outliers_iqr_capped.sum()

print("Number of remaining outliers after capping per column:")
print(remaining_outlier_counts_capped)

-------------still getting outliers, therefore applying logtransformation---------

    df_log_transformed = df_replaced.copy()
for col in columns_of_interest:
    df_log_transformed[col] = np.log1p(df_log_transformed[col])

# Check for outliers after log transformation
Q1_log = df_log_transformed[numerical_vars].quantile(0.25)
Q3_log = df_log_transformed[numerical_vars].quantile(0.75)
IQR_log = Q3_log - Q1_log

outliers_iqr_log = (df_log_transformed[numerical_vars] < (Q1_log - 1.5 * IQR_log)) | (df_log_transformed[numerical_vars] > (Q3_log + 1.5 * IQR_log))
remaining_outlier_counts_log = outliers_iqr_log.sum()

print("Number of remaining outliers after log transformation per column:")
print(remaining_outlier_counts_log)

    -------Cap the remaining outliers after log transformation-------

    df_capped_log = df_log_transformed.copy()
for col in columns_of_interest:
    if col in outliers_iqr_log.columns:
        lower_bound = Q1_log[col] - 1.5 * IQR_log[col]
        upper_bound = Q3_log[col] + 1.5 * IQR_log[col]
        df_capped_log[col] = df_capped_log[col].clip(lower=lower_bound, upper=upper_bound)

# Check if any outliers remain after capping
Q1_capped_log = df_capped_log[numerical_vars].quantile(0.25)
Q3_capped_log = df_capped_log[numerical_vars].quantile(0.75)
IQR_capped_log = Q3_capped_log - Q1_capped_log

outliers_iqr_capped_log = (df_capped_log[numerical_vars] < (Q1_capped_log - 1.5 * IQR_capped_log)) | (df_capped_log[numerical_vars] > (Q3_capped_log + 1.5 * IQR_capped_log))
remaining_outlier_counts_capped_log = outliers_iqr_capped_log.sum()

print("Number of remaining outliers after capping post-log transformation per column:")
print(remaining_outlier_counts_capped_log)
----------------------------  correlation --------------------------
2.3 Diagnose for correlations between variables and determine independent and dependent variables

correlation_matrix = df[numerical_vars].corr()
correlation_matrix

------------visualize the correlation with Heatmap, to check for multicollinearity-----------

plt.figure(figsize=(15, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

#Extract independent variables with a correlation coefficient greater than 0.5 or less than -0.5 with the target variable 'Price' to build the predictive model.
correlation_with_price = correlation_matrix['price'].sort_values(ascending=False)
print(correlation_with_price)

-------Select variables with strong correlation to price--------------
strong_correlations = correlation_with_price[abs(correlation_with_price) > 0.5]
print(strong_correlations)

----------------defining independent variables--------

    # Define our independent variables
X = df_final[['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms']]

# Add a constant (intercept) to the model
X = add_constant(X)

# Check for and handle NaN and inf values
X_cleaned = X.replace([np.inf, -np.inf], np.nan).fillna(X.median())

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data['Feature'] = X_cleaned.columns
vif_data['VIF'] = [variance_inflation_factor(X_cleaned.values, i) for i in range(X_cleaned.shape[1])]

print(vif_data)

2.4 Perform exploratory analysis in combination with visualization techniques to discover patterns and features of interest
---------------------checking for multicollinearity using scatter plot----------
 
    selected_columns = X_cleaned.columns
for col1 in selected_columns:
    for col2 in selected_columns:
        if col1 != col2:
            plt.figure(figsize=(8, 6))
            sns.scatterplot(x=X_cleaned[col1], y=X_cleaned[col2])
            plt.title(f'{col1} vs {col2}')
            plt.xlabel(col1)
            plt.ylabel(col2)
            plt.show()

-------------Combining variables ----------
#since there are alot of correlations lets, combine and take average of sqft_living and sqft_living15, since they measure the same concept 
correlation = X_cleaned[['sqft_living', 'sqft_living15']].corr().iloc[0, 1]
print(f"Correlation between sqft_living and sqft_living15: {correlation}")

--------------------combine and drop variables -------------

print(X_cleaned.columns)

X_cleaned['mean_sqft'] = X_cleaned[['sqft_living', 'sqft_living15']].mean(axis=1)

X_cleaned = X_cleaned.drop(['sqft_living', 'sqft_living15'], axis=1)

print(X_cleaned.head())

------------------check the new vif---------------
X_with_const = add_constant(X_cleaned)

vif_data = pd.DataFrame()
vif_data['Feature'] = X_with_const.columns
vif_data['VIF'] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]

print(vif_data)
    
# then we have choosen our predictors----------
X_predictors = ['mean_sqft', 'grade', 'sqft_above',  'bathrooms']


-----------------------------------------------------------------------------------------------------------

*BASED OFF DF ALREADY BEING CLEANED AND IND/DEP VARIABLES BEING DETERIMED



3. Creating a regression model
----------------------------fit the model--------------------------------------------------------
Find the best-fitting line through the data points

model = sm.ols('price ~ sqft_living', data = df).fit()

model1 = sm.ols('price ~ bathrooms', data = df).fit()

model2 = sm.ols('price ~ sqft_above', data = df).fit()

----------------------------print the summary statistics--------------------------
Show a comprehensive summary of the regression model  to understand the relationship between the independent and dependent 
variable 

print(model.summary())

----------------------------calculate predictions--------------------------
Generate predictions based on the fitted model and calculates the predicted value of price based 'sqft_living' using the
regression model

predictions = model.predict(df[['sqft_living']])

predictions1 = model1.predict(df[['bathrooms']])

predictions2 = model2.predict(df[['sqft_above']])

----------We did 1:1 analysis with indepedent and dependent varaible now lets do  analysis with combined cofounding variables to see the effect in price------------

model = sm.ols('price ~ bedrooms + bathrooms+ sqft_living + sqft_lot +floors + waterfront', data = df).fit()
print(model.summary())  

---------------- Since r-sqaure value is not looking good so we are changing the varaibles again----------------
model = sm.ols('price ~ bedrooms + bathrooms+ sqft_living + view + condition + sqft_lot + waterfront +grade + yr_built  ', data = df).fit()
print(model.summary())

----combined variable is giving better r-square values. so, good model for price would be considering bedrooms,bathrooms,sqft_living,view,condition,sqft_lot,waterfront,grade and yr_built 


    
----------------------------calculate evaluation metrics----------------------------------
Asses accuracy of model's predictions using evaluation metrics

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

#price ~ sqft_living
mae = mean_absolute_error(df['price'], predictions)
mse = mean_squared_error(df['price'], predictions)
r2 = r2_score(df['price'], predictions)
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"R² Score: {r2}")

#price ~ bathroom
mae1 = mean_absolute_error(df['price'], predictions1)
mse1 = mean_squared_error(df['price'], predictions1)
r21 = r2_score(df['price'], predictions1)
print(f"Mean Absolute Error (MAE): {mae1}")
print(f"Mean Squared Error (MSE): {mse1}")
print(f"R² Score: {r21}")

#price ~ sqft_above
mae2 = mean_absolute_error(df['price'], predictions2)
mse2 = mean_squared_error(df['price'], predictions2)
r22 = r2_score(df['price'], predictions2)
print(f"Mean Absolute Error (MAE): {mae2}")
print(f"Mean Squared Error (MSE): {mse2}")
print(f"R² Score: {r22}")
------------------------------create scatter plot------------------------------------------------------------------------
Create Scatter Plot of Actual Vs Predicted Prices

plt.figure(figsize=(10, 6))
plt.scatter(df['price'],predictions)
plt.plot([df['price'].min(), df['price'].max()], [df['price'].min(), df['price'].max()], color='red', linestyle='--')
plt.title('Actual vs. Predicted House Prices Based on Living Area (sqft)')
plt.xlabel('Actual Price')
plt.ylabel('predicted price')
plt.show

-----------------------------(optional) make x and y axis show full numbers instead of scientific notation------------------

import matplotlib.ticker as ticker

formatter = ticker.ScalarFormatter(useOffset=False)
formatter.set_scientific(False)

plt.gca().xaxis.set_major_formatter(formatter)
plt.gca().yaxis.set_major_formatter(formatter)

plt.show()



